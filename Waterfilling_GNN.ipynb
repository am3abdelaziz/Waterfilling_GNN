{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waterfilling approximation using GNN \n",
    "\n",
    "In this notebook, we are going to examine a simple application of GNNs in wireless communications which is function approximation. Typically, the problem of power distribution among a set of users can be defined as: \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    " \\max_{p_u} &\\sum_u w_u \\log(1+p_u G_u/\\sigma^2) \\\\\n",
    " subject ~to:& \\sum_u p_u \\leq P \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where, P is the total power budget, $\\sigma^2$ is the noise power ,$G_u$ and $w_u$ is the channel gain  and weight of  user $u$ , respectively. \n",
    "\n",
    "The solution of the above problem is simply given by the well-known waterfilling algorithm, derived from the Lagrangian of the above problem. The power per user is defined as:\n",
    "\n",
    "$p_u = [\\frac{w_u}{\\mu} - \\frac{1}{G_u}]^+$,\n",
    "where $[.]^+ = max(0,.)$ Thus, we only need to solve the non-linear equation:\n",
    "\n",
    "\\begin{equation}\n",
    " \\sum_u [\\frac{w_u}{\\mu} - \\frac{1}{G_u}]^+ = P,\n",
    "\\end{equation}\n",
    "\n",
    "which is done simply using a bisection method, or a backtracking method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to approximate the waterfilling algorithm with a GNN, we will implement the following steps: \n",
    "\n",
    "1. Define the waterfilling function \n",
    "2. Use the function to generate a dataset ( a list of graphs) \n",
    "3. Create a dataloader, Define the model and training parameters\n",
    "4. Train the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Waterfilling function implemenation \n",
    "\n",
    "For simplicity, we will normalize the variables $p_u$ by premultiplying the channel gains $G_u$ by $P/\\sigma^2$ in the below implemenation. We will use torch for all numerical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def waterfilling_backtracking(user_weights,channel_gains):\n",
    "    num_users = len(user_weights)\n",
    "    I_u = torch.ones(num_users) # indicator of which users has a non-zero power\n",
    "    total_power = 1 \n",
    "    while torch.sum(I_u) > 0 : \n",
    "       inv_mu = (total_power + torch.sum(I_u * 1/channel_gains))/torch.sum(I_u * user_weights)\n",
    "       allocated_power = I_u * (user_weights*inv_mu - 1/channel_gains)\n",
    "       # determine set of users assigned a negative rate\n",
    "       Sinactive = torch.where(allocated_power < 0 )[0]\n",
    "       if len(Sinactive) == 0 :\n",
    "           break\n",
    "       I_u[Sinactive] = 0 \n",
    "    return allocated_power\n",
    "\n",
    "\n",
    "\n",
    "def waterfilling_bisection(user_weights,channel_gains,epsilon = 1e-6):\n",
    "    total_power = 1 \n",
    "    mu_min = 0.0  # this waterlevel results in a feasible allocation, yet sub-optimal \n",
    "    mu_max = (total_power + torch.sum(1/channel_gains))/torch.sum(user_weights) + 0.1 # this infeasible waterlevel\n",
    "    num_users = len(user_weights) \n",
    "    \n",
    "    while mu_max- mu_min > epsilon:\n",
    "        mu = (mu_min+mu_max) / 2\n",
    "        allocated_power = torch.maximum(torch.zeros(num_users),user_weights* mu - 1/channel_gains)\n",
    "        power_sum = torch.sum(allocated_power)\n",
    "\n",
    "        if power_sum <= total_power:\n",
    "            mu_min = mu \n",
    "        else: \n",
    "            mu_max = mu\n",
    "        \n",
    "        return allocated_power\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate a dataset of sample realization and the corresponding optimal solution \n",
    "\n",
    "We will use a simplified channel model as the goal of this exercise is to present the feasibiltiy of this approach. We define below a function to generate a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data \n",
    "\n",
    "def generate_sample(_):\n",
    "    min_users = 5 \n",
    "    max_users = 70 \n",
    "    \n",
    "    total_power = 40 \n",
    "    noise_power = 1e-5\n",
    "\n",
    "    num_users = torch.randint(min_users,max_users+1,size=()) \n",
    "    user_weights = torch.rand(num_users)*0.9 + 0.1 \n",
    "    channel_gains = torch.empty(num_users).exponential_(1.0)\n",
    "    \n",
    "    channel_gains = channel_gains * total_power/noise_power\n",
    "    \n",
    "    allocated_power  = waterfilling_backtracking(user_weights,channel_gains)\n",
    "    \n",
    "\n",
    "    edge_index = torch.tensor([(i,j) for i in range(num_users) for j in range(num_users) if i != j],dtype= torch.long).T\n",
    "    \n",
    "    data_graph = Data(\n",
    "        num_nodes = num_users,\n",
    "        w = user_weights,\n",
    "        g = channel_gains,\n",
    "        p = allocated_power ,\n",
    "        edge_index = edge_index\n",
    "          )\n",
    "    data_graph.validate()\n",
    "    return data_graph\n",
    "\n",
    "num_samples = 10000\n",
    "\n",
    "graph_dataset = [ ]\n",
    "for i in range(num_samples):\n",
    "    graph_dataset.append(generate_sample(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: prepare for training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a dataloader to pass data one graph at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "batch_size = 1\n",
    "my_dataloader = DataLoader(graph_dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we define the model to be used. We will use a graph convolutional neural network for its simplicty and since it will support any number of users as with the original algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self,in_channels,hidden_channels,out_channels):\n",
    "        super(GCNModel,self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels,hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels,out_channels)\n",
    "    def forward(self,x1,x2,edge_index):\n",
    "        x = torch.stack((x1,x2),dim = -1)\n",
    "        x = self.conv1(x,edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x,edge_index)\n",
    "        x =  F.softmax(x,dim=0)\n",
    "        \n",
    "        return x \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model, specify the loss function and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCNModel(in_channels=2,hidden_channels=16,out_channels=1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0001)\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: training loop\n",
    "\n",
    "In order to assess the quality of the model pre-training, we will generate a single batch, pass it trough the model and get the output weighted sum-rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 1190], num_nodes=35, w=[35], g=[35], p=[35], batch=[35], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "def compute_weighted_rate(channel_gains, user_weights,allocated_power):\n",
    "    SNR = channel_gains* allocated_power\n",
    "    #print(f'SNR shape is: {SNR.shape}')\n",
    "    weighted_rate = user_weights * torch.log2(1+SNR)\n",
    "    return torch.sum(weighted_rate)\n",
    "\n",
    "\n",
    "batch = next(iter(my_dataloader))\n",
    "print(batch)\n",
    "output = model(batch.w,batch.g,batch.edge_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WSR with ML is 325.9265 and the ideal is 329.4325 \n"
     ]
    }
   ],
   "source": [
    "wsr_ideal = compute_weighted_rate(channel_gains =batch.g,\n",
    "                      user_weights = batch.w,\n",
    "                      allocated_power = batch.p)\n",
    "wsr_model = compute_weighted_rate(channel_gains =batch.g,\n",
    "                      user_weights = batch.w,\n",
    "                      allocated_power = torch.squeeze(output),\n",
    "                      )\n",
    "\n",
    "print(f' WSR with ML is {wsr_model.item():.4f} and the ideal is {wsr_ideal:.4f} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training loop using supervised learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, loss: 0.0007, ideal WSR 325.8770, avg_wsr_ML 322.3577, optimality gap is: 1.0799 %\n",
      "Epoch 2/5, loss: 0.0007, ideal WSR 325.8767, avg_wsr_ML 322.3663, optimality gap is: 1.0772 %\n",
      "Epoch 3/5, loss: 0.0007, ideal WSR 325.8760, avg_wsr_ML 322.3684, optimality gap is: 1.0764 %\n",
      "Epoch 4/5, loss: 0.0007, ideal WSR 325.8761, avg_wsr_ML 322.3569, optimality gap is: 1.0799 %\n",
      "Epoch 5/5, loss: 0.0007, ideal WSR 325.8768, avg_wsr_ML 322.3648, optimality gap is: 1.0777 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0 \n",
    "    total_wsr_ideal = 0 \n",
    "    total_wsr_ML = 0\n",
    "\n",
    "    for batch in my_dataloader: \n",
    "        output = model(batch.w,batch.g,batch.edge_index)\n",
    "\n",
    "        loss = criterion(output,torch.reshape(batch.p,output.shape))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        wsr_ideal = compute_weighted_rate(channel_gains =batch.g,\n",
    "                      user_weights = batch.w,\n",
    "                      allocated_power = batch.p)\n",
    "        wsr_model = compute_weighted_rate(channel_gains =torch.reshape(batch.g,output.shape),\n",
    "                      user_weights = torch.reshape(batch.w,output.shape),\n",
    "                      allocated_power = output,\n",
    "                      )\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_wsr_ideal += wsr_ideal\n",
    "        total_wsr_ML += wsr_model.item()\n",
    "\n",
    "    avg_loss = total_loss/len(my_dataloader)\n",
    "    avg_wsr_ideal = total_wsr_ideal/len(my_dataloader)\n",
    "    avg_wsr_ML = total_wsr_ML/len(my_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, loss: {avg_loss:.4f}, ideal WSR {avg_wsr_ideal:.4f}, avg_wsr_ML {avg_wsr_ML:.4f}, optimality gap is: {(avg_wsr_ideal-avg_wsr_ML)/avg_wsr_ideal*100:.4f} %')\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use self-supervised learning, i.e., learning form the weighted sum-rate instead of learning form the MMSE criterion with the actual output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, loss: 0.0105, ideal WSR 325.8770, avg_wsr_ML 322.3629, optimality gap is: 1.0784 %\n",
      "Epoch 2/5, loss: 0.0105, ideal WSR 325.8763, avg_wsr_ML 322.3602, optimality gap is: 1.0790 %\n",
      "Epoch 3/5, loss: 0.0105, ideal WSR 325.8756, avg_wsr_ML 322.3607, optimality gap is: 1.0786 %\n",
      "Epoch 4/5, loss: 0.0105, ideal WSR 325.8770, avg_wsr_ML 322.3595, optimality gap is: 1.0794 %\n",
      "Epoch 5/5, loss: 0.0105, ideal WSR 325.8767, avg_wsr_ML 322.3603, optimality gap is: 1.0791 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0 \n",
    "    total_wsr_ideal = 0 \n",
    "    total_wsr_ML = 0\n",
    "\n",
    "    for batch in my_dataloader: \n",
    "        output = model(batch.w,batch.g,batch.edge_index)\n",
    "\n",
    "        wsr_ideal = compute_weighted_rate(channel_gains =batch.g,\n",
    "                      user_weights = batch.w,\n",
    "                      allocated_power = batch.p)\n",
    "        wsr_model = compute_weighted_rate(channel_gains =torch.reshape(batch.g,output.shape),\n",
    "                      user_weights = torch.reshape(batch.w,output.shape),\n",
    "                      allocated_power = output,\n",
    "                      )\n",
    "        \n",
    "        loss = 1 - wsr_model/wsr_ideal \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_wsr_ideal += wsr_ideal\n",
    "        total_wsr_ML += wsr_model.item()\n",
    "\n",
    "    avg_loss = total_loss/len(my_dataloader)\n",
    "    avg_wsr_ideal = total_wsr_ideal/len(my_dataloader)\n",
    "    avg_wsr_ML = total_wsr_ML/len(my_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, loss: {avg_loss:.4f}, ideal WSR {avg_wsr_ideal:.4f}, avg_wsr_ML {avg_wsr_ML:.4f}, optimality gap is: {(avg_wsr_ideal-avg_wsr_ML)/avg_wsr_ideal*100:.4f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wireless_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
